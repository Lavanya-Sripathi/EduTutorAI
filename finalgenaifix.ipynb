{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff74cdce58b34c1881f6b92e38fd89d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_415a2949303a48ba91a9ed97f74c9f0b",
              "IPY_MODEL_abf502f264c946c78a45f6d3114f263b",
              "IPY_MODEL_560a737620b84e0d9c5630ace6bd2480"
            ],
            "layout": "IPY_MODEL_7cf28d8136454d988fadbb4e7c345169"
          }
        },
        "415a2949303a48ba91a9ed97f74c9f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41982e6b82624bffa68463215c318df7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8814c53ad27445b088081db2ec5478e2",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "abf502f264c946c78a45f6d3114f263b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4427b6acd9940358818bcabf57bd859",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0743361638b34a3b9c1dc52610e1146d",
            "value": 2
          }
        },
        "560a737620b84e0d9c5630ace6bd2480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0ca6011da2d4508bb28b771b2220308",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d8ca5f1ab38d4b9a9042de24f9e4cb60",
            "value": "‚Äá2/2‚Äá[00:31&lt;00:00,‚Äá12.91s/it]"
          }
        },
        "7cf28d8136454d988fadbb4e7c345169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41982e6b82624bffa68463215c318df7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8814c53ad27445b088081db2ec5478e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4427b6acd9940358818bcabf57bd859": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0743361638b34a3b9c1dc52610e1146d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0ca6011da2d4508bb28b771b2220308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ca5f1ab38d4b9a9042de24f9e4cb60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdBH6S_Sqe39",
        "outputId": "31421d4b-335d-44a7-8f79-db914749767d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XayDOFCzqkev",
        "outputId": "0499f0b4-0a05-4007-b273-7e8f64d945bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "import torch\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "# Dummy user database for login authentication\n",
        "# Initialize users_db outside of the main Gradio launch block\n",
        "# Check if users_db is already defined to prevent resetting on cell re-execution\n",
        "if 'users_db' not in globals():\n",
        "    users_db = {\"student1\": \"pass123\", \"student2\": \"abc456\"}\n",
        "\n",
        "\n",
        "# Store quiz attempts and classroom tracking\n",
        "user_sessions = {}\n",
        "\n",
        "# Step 1: Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device set to use {device}\")\n",
        "\n",
        "# Step 2: Load model & tokenizer\n",
        "try:\n",
        "    model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # Removed load_in_4bit=True to address bitsandbytes error\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=700\n",
        "    )\n",
        "    print(\"‚úÖ Model and tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model/tokenizer: {e}\")\n",
        "    generator = None\n",
        "\n",
        "# Utility function to generate text\n",
        "def generate_response(prompt):\n",
        "    if generator is None:\n",
        "        print(\"‚ùå Error in generate_response: Model not loaded.\")\n",
        "        return \"‚ùå Error: Model not loaded.\"\n",
        "    try:\n",
        "        print(f\"Generating response for prompt: {prompt[:100]}...\")\n",
        "        response = generator(prompt)\n",
        "        if response and isinstance(response, list) and len(response) > 0 and \"generated_text\" in response[0]:\n",
        "            print(\"‚úÖ Response generated successfully.\")\n",
        "            return response[0][\"generated_text\"]\n",
        "        else:\n",
        "            print(f\"‚ùå Unexpected response format: {response}\")\n",
        "            return \"‚ùå Error: Unexpected response format.\"\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during text generation: {e}\")\n",
        "        return f\"‚ùå Error during text generation: {e}\"\n",
        "\n",
        "# Functionality 1: Concept Understanding\n",
        "def concept_understanding(concept):\n",
        "    prompt = f\"\"\"\n",
        "Explain the concept of '{concept}' in a simple and clear way that a 15-year-old student can easily understand. Include examples and real-world applications if possible.\n",
        "\"\"\"\n",
        "    return generate_response(prompt)\n",
        "\n",
        "# Functionality 2: Language Learning\n",
        "def language_learning(language):\n",
        "    prompt = f\"\"\"\n",
        "Teach me the basics of {language} language. Include grammar rules, common vocabulary, and parts of speech.\n",
        "\"\"\"\n",
        "    return generate_response(prompt)\n",
        "\n",
        "# Functionality 3: Test Generator from PDF\n",
        "def generate_test_from_pdf(pdf_file):\n",
        "    if not pdf_file:\n",
        "        return \"‚ùå Please upload a PDF file.\"\n",
        "    try:\n",
        "        reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \" \".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "        if not text:\n",
        "            return \"‚ùå Could not extract text from PDF.\"\n",
        "        prompt = f\"\"\"\n",
        "Generate a set of multiple-choice questions from the following content:\n",
        "\n",
        "{text}\n",
        "\n",
        "Format each question like this:\n",
        "Qn: <question>\n",
        "A. <option A>\n",
        "B. <option B>\n",
        "C. <option C>\n",
        "D. <option D>\n",
        "Correct Answer: <correct letter>\n",
        "\"\"\"\n",
        "        return generate_response(prompt)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Failed to process PDF or generate test: {e}\"\n",
        "\n",
        "# Quiz Generation\n",
        "def quiz_generator(topic):\n",
        "    prompt = f\"Generate a 5-question multiple choice quiz on the topic: {topic}. Format each question with options A, B, C, D and indicate the Correct Answer: <letter>.\"\n",
        "    quiz_text = generate_response(prompt)\n",
        "    print(f\"Raw Quiz Generator Output: {quiz_text}\") # Add print statement for raw output\n",
        "\n",
        "    # Parse the generated text into a structured format\n",
        "    questions = []\n",
        "    # Split the text by question number, more robust pattern\n",
        "    # This pattern looks for newline followed by a digit (1 or more) followed by a period\n",
        "    question_blocks = re.split(r'\\n\\d+\\.\\s*', quiz_text)\n",
        "\n",
        "    for block in question_blocks:\n",
        "        if not block.strip():\n",
        "            continue\n",
        "\n",
        "        # Extract question text - look for text before the first option (A., B., etc.)\n",
        "        question_match = re.match(r'(.*?)(?=\\n[A-D]\\.\\s*|$)', block, re.DOTALL)\n",
        "        question_text = question_match.group(1).strip() if question_match else \"Could not parse question\"\n",
        "\n",
        "        # Extract options - find all lines starting with A., B., C., or D.\n",
        "        options = re.findall(r'\\n([A-D])\\.\\s*(.*?)(?=\\n[A-D]\\.\\s*|\\nCorrect Answer:|$)', block, re.DOTALL)\n",
        "        options_dict = {opt[0]: opt[1].strip() for opt in options}\n",
        "\n",
        "        # Extract correct answer\n",
        "        correct_answer_match = re.search(r'Correct Answer:\\s*([A-D])', block)\n",
        "        correct_answer = correct_answer_match.group(1) if correct_answer_match else \"N/A\"\n",
        "\n",
        "        # Only add question if successfully parsed and has options\n",
        "        if question_text != \"Could not parse question\" and options_dict:\n",
        "            questions.append({\n",
        "                \"question\": question_text,\n",
        "                \"options\": options_dict,\n",
        "                \"correct_answer\": correct_answer\n",
        "            })\n",
        "    return questions\n",
        "\n",
        "# Auth Logic\n",
        "def authenticate(username, password):\n",
        "    return users_db.get(username) == password\n",
        "\n",
        "def register_user(new_username, new_password):\n",
        "    if new_username in users_db:\n",
        "        return \"‚ùå Username already exists!\"\n",
        "    else:\n",
        "        users_db[new_username] = new_password\n",
        "        return \"‚úÖ User registered successfully. You can now login.\"\n",
        "\n",
        "# Full App Logic\n",
        "def run_all(username, concept, language, pdf_file, quiz_topic): # Added quiz_topic to inputs\n",
        "    if username not in user_sessions:\n",
        "        user_sessions[username] = []\n",
        "\n",
        "    concept_output = concept_understanding(concept)\n",
        "    language_output = language_learning(language)\n",
        "\n",
        "    # Generate test from PDF\n",
        "    test_pdf_output = generate_test_from_pdf(pdf_file) # Keep as raw text output\n",
        "    print(f\"PDF Test Generator Output: {test_pdf_output}\") # Add print statement\n",
        "\n",
        "    # Generate quiz from topic\n",
        "    quiz_data = quiz_generator(quiz_topic) # Generate quiz using the new input\n",
        "\n",
        "    # Format quiz data for display\n",
        "    formatted_quiz_output = \"\"\n",
        "    if quiz_data:\n",
        "        for i, q in enumerate(quiz_data):\n",
        "            formatted_quiz_output += f\"Q{i+1}: {q['question']}\\n\"\n",
        "            for option_key, option_value in q['options'].items():\n",
        "                formatted_quiz_output += f\"{option_key}. {option_value}\\n\"\n",
        "            formatted_quiz_output += f\"Correct Answer: {q['correct_answer']}\\n\\n\"\n",
        "    else:\n",
        "        formatted_quiz_output = \"Could not generate quiz.\"\n",
        "\n",
        "\n",
        "    # Prepare Gradio outputs - must match the order of outputs in run_btn.click\n",
        "    outputs = [\n",
        "        concept_output, # concept_out\n",
        "        language_output, # language_out\n",
        "        test_pdf_output, # test_out\n",
        "        formatted_quiz_output # quiz_out (formatted string for textbox display)\n",
        "    ]\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# Removed the evaluate_quiz function entirely\n",
        "\n",
        "\n",
        "# Gradio Interface\n",
        "def login_fn(user, pwd):\n",
        "    if authenticate(user, pwd):\n",
        "        return gr.update(visible=True), gr.update(value=\"Login successful. Welcome!\"), user\n",
        "    else:\n",
        "        return gr.update(visible=False), gr.update(value=\"Invalid credentials!\"), \"\"\n",
        "\n",
        "with gr.Blocks() as interface:\n",
        "    gr.Markdown(\"# üë©‚Äçüè´ EduTutor AI: Personalized Learning Platform\")\n",
        "\n",
        "    username_state = gr.State(\"\")\n",
        "    # Removed quiz_questions_state as topic quiz is removed\n",
        "\n",
        "    with gr.Tab(\"Login\"):\n",
        "        login_user = gr.Textbox(label=\"Username\")\n",
        "        login_pwd = gr.Textbox(label=\"Password\", type=\"password\")\n",
        "        login_status = gr.Textbox(label=\"Status\")\n",
        "        login_button = gr.Button(\"Login\")\n",
        "\n",
        "    with gr.Tab(\"Register\"):\n",
        "        new_user = gr.Textbox(label=\"New Username\")\n",
        "        new_pwd = gr.Textbox(label=\"New Password\", type=\"password\")\n",
        "        register_button = gr.Button(\"Register\")\n",
        "        registration_status = gr.Textbox(label=\"Registration Status\")\n",
        "        register_button.click(fn=register_user, inputs=[new_user, new_pwd], outputs=registration_status)\n",
        "\n",
        "\n",
        "    with gr.Tab(\"Classroom\"):\n",
        "        with gr.Column(visible=False) as app_ui:\n",
        "            concept = gr.Textbox(label=\"Enter Concept (e.g., Generative AI)\")\n",
        "            language = gr.Radio(choices=[\"English\", \"Hindi\"], label=\"Choose Language\")\n",
        "            pdf = gr.File(label=\"Upload PDF\")\n",
        "            quiz_topic = gr.Textbox(label=\"Enter Topic for Quiz\") # Added new input field for quiz topic\n",
        "            run_btn = gr.Button(\"Run EduTutor AI\")\n",
        "\n",
        "            concept_out = gr.Textbox(label=\"Concept Explanation\", show_copy_button=True)\n",
        "            language_out = gr.Textbox(label=\"Language Learning\", show_copy_button=True)\n",
        "            test_out = gr.Textbox(label=\"Generated Test from PDF\", show_copy_button=True) # Keep PDF test output\n",
        "            quiz_out = gr.Textbox(label=\"Generated Quiz\", show_copy_button=True) # Added new output field for the quiz\n",
        "\n",
        "\n",
        "            # Removed quiz_topic_section and its components\n",
        "\n",
        "\n",
        "            run_btn.click(fn=run_all,\n",
        "                         inputs=[username_state, concept, language, pdf, quiz_topic], # Added quiz_topic to inputs\n",
        "                         outputs=[concept_out, language_out, test_out, quiz_out]) # Added quiz_out to outputs\n",
        "\n",
        "        login_button.click(fn=login_fn, inputs=[login_user, login_pwd], outputs=[app_ui, login_status, username_state])\n",
        "\n",
        "\n",
        "interface.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ff74cdce58b34c1881f6b92e38fd89d4",
            "415a2949303a48ba91a9ed97f74c9f0b",
            "abf502f264c946c78a45f6d3114f263b",
            "560a737620b84e0d9c5630ace6bd2480",
            "7cf28d8136454d988fadbb4e7c345169",
            "41982e6b82624bffa68463215c318df7",
            "8814c53ad27445b088081db2ec5478e2",
            "d4427b6acd9940358818bcabf57bd859",
            "0743361638b34a3b9c1dc52610e1146d",
            "f0ca6011da2d4508bb28b771b2220308",
            "d8ca5f1ab38d4b9a9042de24f9e4cb60"
          ]
        },
        "id": "QbaDzwc0coX2",
        "outputId": "9ce0edea-e18c-416a-eea0-dc8168ea7fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff74cdce58b34c1881f6b92e38fd89d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and tokenizer loaded successfully.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://eabadb171932e54691.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eabadb171932e54691.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating response for prompt: \n",
            "Explain the concept of 'what is cybersecurity?' in a simple and clear way that a 15-year-old studen...\n",
            "‚úÖ Response generated successfully.\n",
            "Generating response for prompt: \n",
            "Teach me the basics of English language. Include grammar rules, common vocabulary, and parts of spe...\n",
            "‚úÖ Response generated successfully.\n",
            "Generating response for prompt: \n",
            "Generate a set of multiple-choice questions from the following content:\n",
            "\n",
            " \n",
            "Page 1 of 3 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Vid...\n",
            "‚úÖ Response generated successfully.\n",
            "PDF Test Generator Output: \n",
            "Generate a set of multiple-choice questions from the following content:\n",
            "\n",
            " \n",
            "Page 1 of 3 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Video transcript  \n",
            " \n",
            "How large  language models work \n",
            " \n",
            "GPT  or generative pre trained transformer is a  large language model or an LLM that can \n",
            "generate  human like text and I've been using GPT  in its  various forms for years . In this video , \n",
            "we are going to number one , ask what is an LLM ? Number  two, we are going to describe how \n",
            "they work . And then number three , we're going to  ask what are  the business applications of \n",
            "LLMs.  \n",
            " \n",
            "So let's start with number one , what is a large  language model . Well, a large language model  is \n",
            "an instance of something else , called a  foundation model . Now , foundation models are pre \n",
            "trained on large amounts of  unlabeled and self-supervised  data, meaning the model learns \n",
            "from  patterns in the data in the way that produces  generalizable  and adaptable output . \n",
            " \n",
            "And large language models are instances of foundation models  applied specifically to text  and \n",
            "text like things . I'm talking about things  \n",
            "like code . Now,  large language models are trained on  large data sets of text such  as books , \n",
            "articles , and conversations  and look when we say large these models can be  tens of gigabytes  \n",
            "in size and trained on enormous amounts of  text data . We're talking potentially  petabyte s of \n",
            "data here . So to put that into perspective , a text file that is let's say one gigabyte  in size , that \n",
            "can store about one hundred  and seventy eight million  words . A lot of words just in one GB. \n",
            "And how many gigabyte s are in a petabyte ? Well, it's about one million yet that's truly a lot of \n",
            "text.  \n",
            " \n",
            "LLS are also among the  biggest models when it comes to parameter  count.  A parameter  is a \n",
            "value the model can change independently.  \n",
            "as it learns and the more parameters a model has the more complex  it can be . GPT-3 for \n",
            "example is  pre trained on a corpus of actually forty -five terabytes  of data and it uses a hundred  \n",
            "and seventy -five billion LLM  parameters .  \n",
            " \n",
            "All right , so how do they work ? Well, we can think of it like this . LLM  equals three things  ‚Äì data , \n",
            "architecture, and lastly , we can think of it  as training . Those three things  are really the \n",
            "components of an LLM.  Now, we've already discussed the enormous amounts of  text data that \n",
            "goes into these things . As for the architecture , this is a neural network  and for  GPT, that is a \n",
            "tran sformer.  \n",
            " \n",
            "And the transformer architecture enables the model to handl e sequences of data like \n",
            "sentences or lines of code , and transformers are designed to understand the  context of each \n",
            "word in a sentence by considering it  \n",
            "   \n",
            "Page 2 of 3 \n",
            " \n",
            "in relation to every other word . This allows the model to build a  comprehensive understanding \n",
            "of the sentence structure  and the meaning of the words within it . And then this architecture is \n",
            "trained on  all of this large amount of data . During training , the model learns  to predict  the next \n",
            "word in a sentence . So, ‚Äúthe sky is ‚Äù it starts off with a random  guess . ‚ÄúThe sky is bug ‚Äù. But with \n",
            "each iteration , the model adjusts its internal parameters  to reduce the difference between its \n",
            "predictions  and the actual outcomes . And the model keeps doing this gradually improving its \n",
            "word  predictions until it can reliably generate  coherent sentences . Forget about  ‚Äúbug‚Äù, it can \n",
            "figure out its ‚Äúblue ‚Äù.  \n",
            " \n",
            "Now,  the model can be fine-tuned  on a  smaller more specific data set . Here the model  refines \n",
            "its understanding to be  able to perform this specific task more accurately . Fine tuning  is what \n",
            "allows a general language model  to become an expert at a specific task .  \n",
            " \n",
            "Okay, so how does this all fit in to number three , business applications ? Well, for customer \n",
            "service applications , businesses can  \n",
            "use LLMs  to create intelligent chat bots that can a variety of customer queries freeing up  human \n",
            "agents for more complex issues . \n",
            " \n",
            "Another good field is content creation . That can benefit from LLMs  which can help  generate \n",
            "articles , emails , social media posts, and  even YouTube  video scripts . Hm, there's an idea now .  \n",
            " \n",
            "LLMs  can even contribute to  software development,  and they can do that by  helping to \n",
            "generate and review code . And  that's just scratching  the surface . As large language models \n",
            "continue to evolve , we're  bound to discover more innovative applications  and that's why  I‚Äôm so \n",
            "enamored  with large  language models .  \n",
            " \n",
            "If you have any questions,  please drop us  a line below and if you want to see more videos  like \n",
            "this in the future , please  like and subscribe . \n",
            " \n",
            "Thanks  for watching . \n",
            " \n",
            "\n",
            "Format each question like this:\n",
            "Qn: <question>\n",
            "A. <option A>\n",
            "B. <option B>\n",
            "C. <option C>\n",
            "D. <option D>\n",
            "Correct Answer: <correct letter>\n",
            "\n",
            "Qn: What is a large language model, according to the video transcript?\n",
            "A. A type of artificial intelligence model used for image recognition\n",
            "B. An instance of a foundation model pre-trained on large, unlabeled, and self-supervised data to generate human-like text\n",
            "C. A machine learning model designed for predicting stock market trends\n",
            "D. A software tool for automating social media posts\n",
            "Correct Answer: B\n",
            "\n",
            "Page 2 of 3 \n",
            "\n",
            "Qn: How large are the models mentioned in the video, in terms of text data?\n",
            "A. A few kilobytes\n",
            "B. Tens of gigabytes\n",
            "C. Petabytes\n",
            "D. Gigabytes\n",
            "Correct Answer: C\n",
            "\n",
            "Qn: What type of neural network architecture is used in large language models like GPT?\n",
            "A. Convolutional Neural Network\n",
            "B. Recurrent Neural Network\n",
            "C. Transformer\n",
            "D. Autoencoder\n",
            "Correct Answer: C\n",
            "\n",
            "Qn: How does a large language model like GPT learn to generate text?\n",
            "A. It learns by reading a fixed set of rules\n",
            "B. It learns by predicting the next word in a sentence\n",
            "C. It learns through reinforcement learning\n",
            "D. It learns by analyzing physical laws\n",
            "Correct Answer: B\n",
            "\n",
            "Qn: What is fine-tuning in the context of large language models?\n",
            "A. Retraining the model from scratch with new data\n",
            "B. Adjusting the model's parameters to improve performance on a specific task\n",
            "C. Increasing the model's complexity without changing its architecture\n",
            "D. Decreasing the model's input size to fit into memory\n",
            "Correct Answer: B\n",
            "\n",
            "Qn: What are some business applications of large language models, as mentioned in the video?\n",
            "A. Developing autonomous vehicles and drones\n",
            "B. Implementing AI-driven fraud detection systems\n",
            "C. Creating intelligent chatbots for customer service and content creation\n",
            "D. Automating data entry processes\n",
            "Correct Answer: C\n",
            "\n",
            "Qn: How can large language models contribute to software development?\n",
            "A. By writing entire software applications from scratch\n",
            "B. By generating and reviewing code snippets for specific tasks\n",
            "C. By designing and implementing the entire software architecture\n",
            "D. By executing software programs without human intervention\n",
            "Correct Answer: B\n",
            "\n",
            "Page 3 of 3 \n",
            "\n",
            "Note: The above questions and answers are generated based on the provided text and might need adjustments depending on the specific context or additional information from the video.\n",
            "Generating response for prompt: Generate a 5-question multiple choice quiz on the topic: encoders and decoders in gen AI. Format eac...\n",
            "‚úÖ Response generated successfully.\n",
            "Raw Quiz Generator Output: Generate a 5-question multiple choice quiz on the topic: encoders and decoders in gen AI. Format each question with options A, B, C, D and indicate the Correct Answer: <letter>.\n",
            "\n",
            "1. What is the primary function of an encoder in gen AI?\n",
            "   A. To decode data into human-readable format.\n",
            "   B. To compress and condense data into a more manageable form.\n",
            "   C. To convert categorical data into numerical data.\n",
            "   D. To directly translate text from one language to another.\n",
            "   Correct Answer: C\n",
            "\n",
            "2. Which of the following is NOT a role of a decoder in gen AI?\n",
            "   A. To perform dimensionality reduction.\n",
            "   B. To detect anomalies in encoded data.\n",
            "   C. To convert numerical data back into its original form.\n",
            "   D. To generate synthetic data using specific models.\n",
            "   Correct Answer: B\n",
            "\n",
            "3. What encoding technique is commonly used in gen AI for handling text data?\n",
            "   A. Binarization\n",
            "   B. Stemming\n",
            "   C. One-hot encoding\n",
            "   D. Embedding\n",
            "   Correct Answer: D\n",
            "\n",
            "4. In gen AI, which method ensures high-level representation of data, making it suitable for tasks such as classification or clustering?\n",
            "   A. Hashing\n",
            "   B. PCA (Principal Component Analysis)\n",
            "   C. Word2Vec\n",
            "   D. All of the above\n",
            "   Correct Answer: C\n",
            "\n",
            "5. Which technique combines encoding and decoding processes to create entirely new data?\n",
            "   A. Autoencoders\n",
            "   B. Generative Adversarial Networks (GANs)\n",
            "   C. Recurrent Neural Networks (RNNs)\n",
            "   D. All of the above\n",
            "   Correct Answer: B\n",
            "\n",
            "---\n",
            "\n",
            "Note: This quiz primarily focuses on fundamental concepts related to encoders and decoders in gen AI. It's designed to test understanding of their primary functions, roles, encoding techniques, and their application in tasks like dimensionality reduction and data generation. The multiple-choice format allows learners to select their most appropriate answer, reinforcing comprehension.\n",
            "Generating response for prompt: \n",
            "Explain the concept of 'what is cybersecurity?' in a simple and clear way that a 15-year-old studen...\n",
            "‚úÖ Response generated successfully.\n",
            "Generating response for prompt: \n",
            "Teach me the basics of English language. Include grammar rules, common vocabulary, and parts of spe...\n",
            "‚úÖ Response generated successfully.\n",
            "Generating response for prompt: \n",
            "Generate a set of multiple-choice questions from the following content:\n",
            "\n",
            " \n",
            "Page 1 of 3 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Vid...\n",
            "‚úÖ Response generated successfully.\n",
            "PDF Test Generator Output: \n",
            "Generate a set of multiple-choice questions from the following content:\n",
            "\n",
            " \n",
            "Page 1 of 3 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Video transcript  \n",
            " \n",
            "How large  language models work \n",
            " \n",
            "GPT  or generative pre trained transformer is a  large language model or an LLM that can \n",
            "generate  human like text and I've been using GPT  in its  various forms for years . In this video , \n",
            "we are going to number one , ask what is an LLM ? Number  two, we are going to describe how \n",
            "they work . And then number three , we're going to  ask what are  the business applications of \n",
            "LLMs.  \n",
            " \n",
            "So let's start with number one , what is a large  language model . Well, a large language model  is \n",
            "an instance of something else , called a  foundation model . Now , foundation models are pre \n",
            "trained on large amounts of  unlabeled and self-supervised  data, meaning the model learns \n",
            "from  patterns in the data in the way that produces  generalizable  and adaptable output . \n",
            " \n",
            "And large language models are instances of foundation models  applied specifically to text  and \n",
            "text like things . I'm talking about things  \n",
            "like code . Now,  large language models are trained on  large data sets of text such  as books , \n",
            "articles , and conversations  and look when we say large these models can be  tens of gigabytes  \n",
            "in size and trained on enormous amounts of  text data . We're talking potentially  petabyte s of \n",
            "data here . So to put that into perspective , a text file that is let's say one gigabyte  in size , that \n",
            "can store about one hundred  and seventy eight million  words . A lot of words just in one GB. \n",
            "And how many gigabyte s are in a petabyte ? Well, it's about one million yet that's truly a lot of \n",
            "text.  \n",
            " \n",
            "LLS are also among the  biggest models when it comes to parameter  count.  A parameter  is a \n",
            "value the model can change independently.  \n",
            "as it learns and the more parameters a model has the more complex  it can be . GPT-3 for \n",
            "example is  pre trained on a corpus of actually forty -five terabytes  of data and it uses a hundred  \n",
            "and seventy -five billion LLM  parameters .  \n",
            " \n",
            "All right , so how do they work ? Well, we can think of it like this . LLM  equals three things  ‚Äì data , \n",
            "architecture, and lastly , we can think of it  as training . Those three things  are really the \n",
            "components of an LLM.  Now, we've already discussed the enormous amounts of  text data that \n",
            "goes into these things . As for the architecture , this is a neural network  and for  GPT, that is a \n",
            "tran sformer.  \n",
            " \n",
            "And the transformer architecture enables the model to handl e sequences of data like \n",
            "sentences or lines of code , and transformers are designed to understand the  context of each \n",
            "word in a sentence by considering it  \n",
            "   \n",
            "Page 2 of 3 \n",
            " \n",
            "in relation to every other word . This allows the model to build a  comprehensive understanding \n",
            "of the sentence structure  and the meaning of the words within it . And then this architecture is \n",
            "trained on  all of this large amount of data . During training , the model learns  to predict  the next \n",
            "word in a sentence . So, ‚Äúthe sky is ‚Äù it starts off with a random  guess . ‚ÄúThe sky is bug ‚Äù. But with \n",
            "each iteration , the model adjusts its internal parameters  to reduce the difference between its \n",
            "predictions  and the actual outcomes . And the model keeps doing this gradually improving its \n",
            "word  predictions until it can reliably generate  coherent sentences . Forget about  ‚Äúbug‚Äù, it can \n",
            "figure out its ‚Äúblue ‚Äù.  \n",
            " \n",
            "Now,  the model can be fine-tuned  on a  smaller more specific data set . Here the model  refines \n",
            "its understanding to be  able to perform this specific task more accurately . Fine tuning  is what \n",
            "allows a general language model  to become an expert at a specific task .  \n",
            " \n",
            "Okay, so how does this all fit in to number three , business applications ? Well, for customer \n",
            "service applications , businesses can  \n",
            "use LLMs  to create intelligent chat bots that can a variety of customer queries freeing up  human \n",
            "agents for more complex issues . \n",
            " \n",
            "Another good field is content creation . That can benefit from LLMs  which can help  generate \n",
            "articles , emails , social media posts, and  even YouTube  video scripts . Hm, there's an idea now .  \n",
            " \n",
            "LLMs  can even contribute to  software development,  and they can do that by  helping to \n",
            "generate and review code . And  that's just scratching  the surface . As large language models \n",
            "continue to evolve , we're  bound to discover more innovative applications  and that's why  I‚Äôm so \n",
            "enamored  with large  language models .  \n",
            " \n",
            "If you have any questions,  please drop us  a line below and if you want to see more videos  like \n",
            "this in the future , please  like and subscribe . \n",
            " \n",
            "Thanks  for watching . \n",
            " \n",
            "\n",
            "Format each question like this:\n",
            "Qn: <question>\n",
            "A. <option A>\n",
            "B. <option B>\n",
            "C. <option C>\n",
            "D. <option D>\n",
            "Correct Answer: <correct letter>\n",
            "\n",
            "Qn: What is an LLM in the context of the video?\n",
            "A. A type of traditional model used for text generation\n",
            "B. An instance of a foundation model pre-trained on large amounts of unlabeled and self-supervised data\n",
            "C. A small-scale language model used for coding\n",
            "D. A model specifically designed for generating human-like text\n",
            "Correct Answer: B\n",
            "\n",
            "Qn: What data does an LLM like GPT-3 use for training?\n",
            "A. A small dataset of handwritten notes\n",
            "B. Large datasets of text such as books, articles, and conversations\n",
            "C. A limited set of coding examples\n",
            "D. A corpus of fifty terabytes\n",
            "Correct Answer: B\n",
            "\n",
            "Qn: How are LLMs trained according to the video?\n",
            "A. By manually inputting text pairs with corresponding translations\n",
            "B. Through iterative adjustments to internal parameters to minimize differences between predictions and actual outcomes\n",
            "C. By using a rule-based system to generate text\n",
            "D. Primarily by a human supervisor reviewing and editing generated text\n",
            "Correct Answer: B\n",
            "\n",
            "Qn: What is the purpose of fine-tuning an LLM according to the video?\n",
            "A. To reduce the size of the model for easier deployment\n",
            "B. To adapt the general understanding of the model to a specific task with a smaller, task-related dataset\n",
            "C. To replace the original training data with new, unrelated data\n",
            "D. To decrease the complexity of the model architecture\n",
            "Correct Answer: B\n",
            "\n",
            "Qn: Which business applications does the video suggest for LLMs?\n",
            "A. Legal services, financial modeling, and data analysis\n",
            "B. Customer service, content creation, and software development\n",
            "C. Medical diagnosis, market research, and database management\n",
            "D. Artificial intelligence research, linguistics studies, and philosophical inquiries\n",
            "Correct Answer: B\n",
            "\n",
            "Qn: Why is the speaker enthusiastic about LLMs according to the video?\n",
            "A. Due to their potential for replacing human labor across all industries\n",
            "B. Because of their ability to innovate across various fields and adapt to new tasks\n",
            "C. Because they can easily replace traditional algorithms with their own capabilities\n",
            "D. Simply because they are the latest tech trend\n",
            "Correct Answer: B\n",
            "Generating response for prompt: Generate a 5-question multiple choice quiz on the topic: transformers. Format each question with opt...\n",
            "‚úÖ Response generated successfully.\n",
            "Raw Quiz Generator Output: Generate a 5-question multiple choice quiz on the topic: transformers. Format each question with options A, B, C, D and indicate the Correct Answer: <letter>.\n",
            "\n",
            "1. What is the primary purpose of a transformer in electrical engineering?\n",
            "A. To increase voltage for transmission loss reduction\n",
            "B. To decrease voltage for consumer use\n",
            "C. To change the frequency of alternating current\n",
            "D. To provide a constant current flow\n",
            "Correct Answer: A.\n",
            "\n",
            "2. Which type of transformer is commonly used in power stations for increasing voltage levels?\n",
            "A. Step-up transformer\n",
            "B. Step-down transformer\n",
            "C. Full-wave transformer\n",
            "D. Island-type transformer\n",
            "Correct Answer: A.\n",
            "\n",
            "3. What phenomenon does a transformer primarily rely on for its operation?\n",
            "A. Electromagnetic induction\n",
            "B. Ohm's law\n",
            "C. Faraday's law of induction\n",
            "D. Kirchhoff's circuit laws\n",
            "Correct Answer: C.\n",
            "\n",
            "4. Which of the following statements about transformers is false?\n",
            "A. Transformers can change the voltage level of an alternating current.\n",
            "B. Transformers do not require a magnetic core for efficient operation.\n",
            "C. Transformers operate based on the principle of electromagnetic induction.\n",
            "D. Transformers are only used in AC circuits.\n",
            "Correct Answer: B.\n",
            "\n",
            "5. What is the ratio of voltage between the primary and secondary coils in a transformer known as?\n",
            "A. Turns ratio\n",
            "B. Power factor\n",
            "C. Impedance\n",
            "D. Efficiency\n",
            "Correct Answer: A.\n",
            "\n",
            "<A></response>\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://eabadb171932e54691.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}